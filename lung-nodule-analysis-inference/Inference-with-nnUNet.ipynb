{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y7HUCTKvL6Ve",
    "outputId": "696f5ad6-0e80-4a70-8421-a49743069100"
   },
   "source": [
    "# Setup\n",
    "## Mount google drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wyxfRPzhMHFm",
    "outputId": "2f260430-0cab-49b7-e317-4d0bde2641ad"
   },
   "source": [
    "!pip install SimpleITK"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "U-urc7VpMHzD",
    "outputId": "35075abe-a586-496a-8b49-aa3a64fdc08a"
   },
   "source": [
    "!pip install nnunetv2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u62jiysEMJn1"
   },
   "source": [
    "# Define folder path\n",
    "folder_path = '/content/drive/My Drive/Inference models balanced dataset'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0nt_QoPMLhT"
   },
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import networks\n",
    "import dataloader\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from typing import Tuple\n",
    "import pandas\n",
    "import scipy.ndimage as snd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nnunetv2.paths import nnUNet_results, nnUNet_raw\n",
    "from batchgenerators.utilities.file_and_folder_operations import join\n",
    "from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO\n",
    "\n",
    "# Helper functions\n",
    "## Function to keep the central connected component in a patch\n",
    "def keep_central_connected_component(\n",
    "    prediction: sitk.Image,\n",
    "    patch_size: Tuple = (128, 128, 64),\n",
    ") -> sitk.Image:\n",
    "    \"\"\"Function to post-process the prediction to keep only the central connected component in a patch\n",
    "\n",
    "    Args:\n",
    "        prediction (sitk.Image): prediction file (should be binary)\n",
    "        patch_size (np.array, optional): patch size (x, y, z) to ensure the center is computed appropriately. Defaults to np.array([96, 96, 96]).\n",
    "\n",
    "    Returns:\n",
    "        sitk.Image: post-processed binary file with only the central connected component\n",
    "    \"\"\"\n",
    "\n",
    "    origin = prediction.GetOrigin()\n",
    "    spacing = prediction.GetSpacing()\n",
    "    direction = prediction.GetDirection()\n",
    "\n",
    "    prediction = sitk.GetArrayFromImage(prediction)\n",
    "\n",
    "    c, n = snd.label(prediction)\n",
    "    centroids = np.array(\n",
    "        [np.array(np.where(c == i)).mean(axis=1) for i in range(1, n + 1)]\n",
    "    ).astype(int)\n",
    "\n",
    "    patch_size = np.array(list(reversed(patch_size)))\n",
    "\n",
    "    if len(centroids) > 0:\n",
    "        dists = np.sqrt(((centroids - patch_size // 2) ** 2).sum(axis=1))\n",
    "        keep_idx = np.argmin(dists)\n",
    "        output = np.zeros(c.shape)\n",
    "        output[c == (keep_idx + 1)] = 1\n",
    "        prediction = output.astype(np.uint8)\n",
    "\n",
    "    prediction = sitk.GetImageFromArray(prediction)\n",
    "    prediction.SetSpacing(spacing)\n",
    "    prediction.SetOrigin(origin)\n",
    "    prediction.SetDirection(direction)\n",
    "    return prediction\n",
    "\n",
    "## Function to perform inference on the test set\n",
    "def perform_inference_on_test_set(workspace: Path):\n",
    "    #check that the workspace is well-defined\n",
    "    if workspace is None:\n",
    "      raise ValueError(\"workspace no puede ser None\")\n",
    "    \n",
    "    #now we load the mdoel architectures for both malignancy and noduletype\n",
    "    malignancy_model = networks.CNN3D(1, 1, task=\"malignancy\").cuda()\n",
    "    noduletype_model = networks.CNN3D(1, 4, task=\"noduletype\").cuda()\n",
    "\n",
    "    #set the malignancy and noduletype models in evaluation mode\n",
    "    malignancy_model.eval()\n",
    "    noduletype_model.eval()\n",
    "\n",
    "    # instantiate the nnUNetPredictor\n",
    "    segmentation_model = nnUNetPredictor(\n",
    "        tile_step_size=0.5,\n",
    "        use_gaussian=True,\n",
    "        use_mirroring=True,\n",
    "        perform_everything_on_device=True,\n",
    "        device=torch.device('cuda', 0),\n",
    "        verbose=False,\n",
    "        verbose_preprocessing=False,\n",
    "        allow_tqdm=True\n",
    "    )\n",
    "    # initializes the network architecture, loads the model parameters\n",
    "    segmentation_model.initialize_from_trained_model_folder(\n",
    "        (workspace / 'nnUNet/nnUNet_results/Dataset001_LUNA/nnUNetTrainer__nnUNetResEncUNetMPlans__3d_fullres'),\n",
    "        use_folds=(\"all\",),\n",
    "        checkpoint_name='checkpoint_best.pth',\n",
    "    )\n",
    "\n",
    "    # load the parameters for malignancy and noduletype models\n",
    "    ckpt = torch.load(workspace / \"20240526_0_malignancy_ORIGINAL/fold0/best_model.pth\")\n",
    "    malignancy_model.load_state_dict(ckpt)\n",
    "\n",
    "    ckpt = torch.load(workspace / \"20240526_0_noduletype_ORIGINAL/fold0/best_model.pth\")\n",
    "    noduletype_model.load_state_dict(ckpt)\n",
    "\n",
    "    #define the paths to the test set and to a reuslts folder to store the predictions\n",
    "    test_set_path = Path(workspace / \"data\" / \"test_set\" / \"images\")\n",
    "    save_path = workspace / \"results\" / \"test_set_predictions\"\n",
    "\n",
    "    segmentation_save_path = save_path / \"segmentations\"\n",
    "    segmentation_save_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "    patch_size = np.array([64, 128, 128])\n",
    "    size_mm = 50\n",
    "    size_px = 64\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # iterate over the images in the test set\n",
    "    for idx, image_path in enumerate(tqdm(list(test_set_path.glob(\"*.mha\")))):\n",
    "\n",
    "        # load and pre-process input image\n",
    "\n",
    "        image = sitk.ReadImage(str(image_path))\n",
    "\n",
    "        #load the image and some properties of it with SimpleITKIO() class from nnunetv2\n",
    "        image_nnunet, props = SimpleITKIO().read_images([str(image_path)])\n",
    "\n",
    "        noduleid = image_path.stem\n",
    "\n",
    "        #image = sitk_image\n",
    "        metad = {\n",
    "            \"origin\": np.flip(image.GetOrigin()),\n",
    "            \"spacing\": np.flip(image.GetSpacing()),\n",
    "            \"transform\": np.array(np.flip(image.GetDirection())).reshape(3, 3),\n",
    "            \"shape\": np.flip(image.GetSize()),\n",
    "        }\n",
    "        image = sitk.GetArrayFromImage(image)\n",
    "\n",
    "        image = image.squeeze()\n",
    "\n",
    "        image = dataloader.extract_patch(\n",
    "            CTData=image,\n",
    "            coord=tuple(patch_size // 2),\n",
    "            srcVoxelOrigin=(0, 0, 0),\n",
    "            srcWorldMatrix=metad[\"transform\"],\n",
    "            srcVoxelSpacing=metad[\"spacing\"],\n",
    "            output_shape=(size_px, size_px, size_px),\n",
    "            voxel_spacing=(\n",
    "                size_mm / size_px,\n",
    "                size_mm / size_px,\n",
    "                size_mm / size_px,\n",
    "            ),\n",
    "            coord_space_world=False,\n",
    "        )\n",
    "        #print(\"Patch extracted succesfully\")\n",
    "\n",
    "        #reshape the image for it to be compatible with the clip and scale function from dataloader\n",
    "        image = image.reshape(1, 1, size_px, size_px, size_px).astype(np.float32)\n",
    "\n",
    "        image = dataloader.clip_and_scale(image)\n",
    "\n",
    "        #convert the image to a tensor and move it to the GPU for it to be processed by malignancy and segmentation models\n",
    "        image_tensor = torch.from_numpy(image).cuda()\n",
    "\n",
    "        #get the outputs of the models\n",
    "        with torch.no_grad():\n",
    "            outputs = {\"segmentation\": segmentation_model.predict_single_npy_array(image_nnunet, props, None, None, False),\n",
    "                #\"segmentation\": segmentation_model(image)[\"segmentation\"],\n",
    "                \"noduletype\": noduletype_model(image_tensor)[\"noduletype\"],\n",
    "                \"malignancy\": malignancy_model(image_tensor)[\"malignancy\"],\n",
    "            }\n",
    "        print(\"Outputs computed by the models\")\n",
    "\n",
    "        outputs = {k: (np.array(outputs[k]).squeeze() if k == \"segmentation\"\n",
    "                       else outputs[k].data.cpu().numpy().squeeze()) for k in outputs.keys()}\n",
    "\n",
    "\n",
    "        segmentation = outputs[\"segmentation\"]\n",
    "\n",
    "        # resample image to original spacing\n",
    "        segmentation = snd.zoom(\n",
    "            segmentation,\n",
    "            (size_mm / size_px) / metad[\"spacing\"],\n",
    "            order=1,\n",
    "        )\n",
    "        #print(f\"{segmentation.shape}\")\n",
    "\n",
    "        # pad image\n",
    "        diff = metad[\"shape\"] - segmentation.shape\n",
    "        #print(diff)\n",
    "        pad_widths = [\n",
    "            (np.round(a), np.round(b))\n",
    "            for a, b in zip(\n",
    "                diff // 2.0 + 1,\n",
    "                diff - diff // 2.0 - 1,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        #if the diff vector has some 0 component then, pad_widths matrix is forced to make 0 all that row, so as \n",
    "        #to keep the shape of the image correctly\n",
    "        for i in range(3):\n",
    "          if diff[i] == 0:\n",
    "            pad_widths[i] = (0.0, 0.0)\n",
    "\n",
    "        pad_widths = np.array(pad_widths).astype(int)\n",
    "        if pad_widths.max() <= 0:\n",
    "          pad_widths[:] = 0\n",
    "\n",
    "        pad_widths = np.clip(pad_widths, 0, pad_widths.max())\n",
    "        pad_widths = np.clip(pad_widths, 0, pad_widths.max())\n",
    "        segmentation = np.pad(\n",
    "            segmentation,\n",
    "            pad_width=pad_widths,\n",
    "            mode=\"constant\",\n",
    "            constant_values=0,\n",
    "        )\n",
    "\n",
    "        # crop, if necessary\n",
    "        if diff.min() < 0:\n",
    "\n",
    "            shape = np.array(segmentation.shape)\n",
    "            center = shape // 2\n",
    "\n",
    "            segmentation = segmentation[\n",
    "                center[0] - patch_size[0] // 2 : center[0] + patch_size[0] // 2,\n",
    "                center[1] - patch_size[1] // 2 : center[1] + patch_size[1] // 2,\n",
    "                center[2] - patch_size[2] // 2 : center[2] + patch_size[2] // 2,\n",
    "            ]\n",
    "\n",
    "        # apply threshold\n",
    "        segmentation = (segmentation > 0.5).astype(np.uint8)\n",
    "\n",
    "        # set metadata\n",
    "        segmentation = sitk.GetImageFromArray(segmentation)\n",
    "        segmentation.SetOrigin(np.flip(metad[\"origin\"]))\n",
    "        segmentation.SetSpacing(np.flip(metad[\"spacing\"]))\n",
    "        segmentation.SetDirection(np.flip(metad[\"transform\"].reshape(-1)))\n",
    "        #print(segmentation.GetSize())\n",
    "\n",
    "        # keep central connected component\n",
    "        segmentation = keep_central_connected_component(segmentation)\n",
    "        print(segmentation.GetSize())\n",
    "        #if segmentation.GetSize()!=(128,128,64):\n",
    "          #break\n",
    "\n",
    "        # write as simpleitk image\n",
    "        sitk.WriteImage(\n",
    "            segmentation,\n",
    "            str(segmentation_save_path / f\"{noduleid}.mha\"),\n",
    "            True,\n",
    "        )\n",
    "\n",
    "        # combine predictions from other task models\n",
    "        prediction = {\n",
    "            \"noduleid\": noduleid,\n",
    "            \"malignancy\": outputs[\"malignancy\"],\n",
    "            \"noduletype\": outputs[\"noduletype\"].argmax(),\n",
    "            \"ggo_probability\": outputs[\"noduletype\"][0],\n",
    "            \"partsolid_probability\": outputs[\"noduletype\"][1],\n",
    "            \"solid_probability\": outputs[\"noduletype\"][2],\n",
    "            \"calcified_probability\": outputs[\"noduletype\"][3],\n",
    "        }\n",
    "\n",
    "        predictions.append(pandas.Series(prediction))\n",
    "\n",
    "    predictions = pandas.DataFrame(predictions)\n",
    "    predictions.to_csv(save_path / \"predictions.csv\", index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2xT2E2w3MOXG",
    "outputId": "737af2c4-93d0-44db-ced0-549c0d9b586a"
   },
   "source": [
    "# Perform inference on the test set\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    workspace = Path(\"/content/drive/My Drive/Inference models balanced dataset\")\n",
    "\n",
    "    perform_inference_on_test_set(workspace=workspace)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
